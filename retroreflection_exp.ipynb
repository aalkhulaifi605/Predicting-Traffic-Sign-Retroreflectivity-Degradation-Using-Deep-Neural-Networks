{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ByuhK7KiWkC-"
      },
      "outputs": [],
      "source": [
        "from dataloader import get_whole_data , get_whole_data_avg\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler , PolynomialFeatures, OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from model import *\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import r2_score, mean_absolute_error , mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "16EW11LmW9oM"
      },
      "outputs": [],
      "source": [
        "def prepare_data(col_to_drop=None):\n",
        "  data = get_whole_data()\n",
        "  drop_col = ['Installation_Year','Replicates']\n",
        "  data.columns = [c.replace(' ', '_') for c in data.columns]\n",
        "  if col_to_drop is not None and col_to_drop != 'Sheeting_Type':\n",
        "    drop_col.append(col_to_drop)\n",
        "  data = data.drop(drop_col,axis=1)\n",
        "  threeM = data.query('Brand == \"3M\"')\n",
        "  threeM_dia = threeM.query('Sheeting_Type == \"Diamond\"')\n",
        "  threeM_hip = threeM.query('Sheeting_Type == \"HIP\"')\n",
        "  threeM_eng = threeM.query('Sheeting_Type == \"Engineering Grade\"')\n",
        "  aviery = data.query('Brand == \"Avery Dennison\"')\n",
        "  aviery_dia = aviery.query('Sheeting_Type == \"Diamond\"')\n",
        "  aviery_hip = aviery.query('Sheeting_Type == \"HIP\"')\n",
        "  aviery_eng = aviery.query('Sheeting_Type == \"Engineering Grade\"')\n",
        "  tong = data.query('Brand == \"TongMing\"')\n",
        "  tong_dia = tong.query('Sheeting_Type == \"Diamond\"')\n",
        "  tong_hip = tong.query('Sheeting_Type == \"HIP\"')\n",
        "  tong_eng = tong.query('Sheeting_Type == \"Engineering Grade\"')\n",
        "  train = pd.concat([threeM_dia,threeM_hip,aviery_eng,tong_hip,tong_eng])\n",
        "  train = pd.concat([threeM_dia,threeM_hip,aviery_eng,tong_hip,tong_eng])\n",
        "\n",
        "  validation =  pd.concat([threeM_eng.iloc[960//2:],aviery_hip,tong_dia.iloc[960//2:]])\n",
        "  test =pd.concat([threeM_eng.iloc[:960//2],aviery_dia,tong_dia.iloc[:960//2]])\n",
        "  if col_to_drop == 'Sheeting_Type':\n",
        "    train = train.drop(['Brand','Sheeting_Type'],axis=1)\n",
        "    validation = validation.drop(['Brand','Sheeting_Type'],axis=1)\n",
        "    test = test.drop(['Brand','Sheeting_Type'],axis=1)\n",
        "  else:\n",
        "    train = train.drop(['Brand',],axis=1)\n",
        "    validation = validation.drop(['Brand'],axis=1)\n",
        "    test = test.drop(['Brand'],axis=1)\n",
        "  return train, validation, test\n",
        "\n",
        "def exp_ohe(col_to_drop=None):\n",
        "  train , validation , test = prepare_data(col_to_drop)\n",
        "  if col_to_drop == None:\n",
        "    t_x = train.iloc[:,0:5]  #independent columns\n",
        "    t_y = train.iloc[:,-1]    #target column \n",
        "    v_x = validation.iloc[:,0:5]  #independent columns\n",
        "    v_y = validation.iloc[:,-1]    #target column \n",
        "    te_x = test.iloc[:,0:5]  #independent columns\n",
        "    te_y = test.iloc[:,-1]    #target column \n",
        "  else:\n",
        "    t_x = train.iloc[:,0:4]  #independent columns\n",
        "    t_y = train.iloc[:,-1]    #target column \n",
        "    v_x = validation.iloc[:,0:4]  #independent columns\n",
        "    v_y = validation.iloc[:,-1]    #target column \n",
        "    te_x = test.iloc[:,0:4]  #independent columns\n",
        "    te_y = test.iloc[:,-1]    #target column \n",
        "  oe = OneHotEncoder()\n",
        "  oe.fit(t_x.to_numpy()[:,:-1])\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  x_enc = oe.transform(t_x.to_numpy()[:,:-1]).toarray()\n",
        "  x_enc = np.append(x_enc, t_x.to_numpy()[:,-1].reshape(len(t_x),1) / 10, axis=1).astype(np.float32)\n",
        "\n",
        "  y_scaled = scaler.fit_transform(t_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  #VALID\n",
        "  vx_enc = oe.transform(v_x.to_numpy()[:,:-1]).toarray()\n",
        "  vx_enc = np.append(vx_enc, v_x.to_numpy()[:,-1].reshape(len(v_x),1) / 10, axis=1).astype(np.float32)\n",
        "  vy_scaled = scaler.fit_transform(v_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  #TEST\n",
        "  tex_enc = oe.transform(te_x.to_numpy()[:,:-1]).toarray()\n",
        "  tex_enc = np.append(tex_enc, te_x.to_numpy()[:,-1].reshape(len(te_x),1) / 10, axis=1).astype(np.float32)\n",
        "  tey_scaled = scaler.fit_transform(te_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  model = create_onehotencode_model2(input=x_enc.shape[-1])\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n",
        "  rlr = ReduceLROnPlateau(patience=1,min_lr=0.00001)\n",
        "  history = model.fit(x_enc,y_scaled, validation_data=(vx_enc, vy_scaled) , batch_size=100,epochs=500,callbacks=[rlr,es],verbose=0)\n",
        "  #TEST RESULTS\n",
        "  prediction = model.predict(tex_enc)\n",
        "  prediction_after_scale = scaler.inverse_transform(prediction)\n",
        "  true_values = scaler.inverse_transform(tey_scaled)\n",
        "  test_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  test_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "  #TRAIN RESULTS\n",
        "  prediction = model.predict(x_enc)\n",
        "  prediction_after_scale = scaler.inverse_transform(prediction)\n",
        "  true_values = scaler.inverse_transform(y_scaled)\n",
        "  train_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  train_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "\n",
        "  #VALIDATION RESULTS\n",
        "  prediction = model.predict(vx_enc)\n",
        "  prediction_after_scale = scaler.inverse_transform(prediction)\n",
        "  true_values = scaler.inverse_transform(vy_scaled)\n",
        "  valid_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  valid_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "  return (train_r2,valid_r2,test_r2) , (train_mae,valid_mae,test_mae)\n",
        "\n",
        "def exp_linear(col_to_drop=None):\n",
        "  train , validation , test = prepare_data(col_to_drop)\n",
        "  if col_to_drop == None:\n",
        "    t_x = train.iloc[:,0:5]  #independent columns\n",
        "    t_y = train.iloc[:,-1]    #target column \n",
        "    v_x = validation.iloc[:,0:5]  #independent columns\n",
        "    v_y = validation.iloc[:,-1]    #target column \n",
        "    te_x = test.iloc[:,0:5]  #independent columns\n",
        "    te_y = test.iloc[:,-1]    #target column \n",
        "  else:\n",
        "    t_x = train.iloc[:,0:4]  #independent columns\n",
        "    t_y = train.iloc[:,-1]    #target column \n",
        "    v_x = validation.iloc[:,0:4]  #independent columns\n",
        "    v_y = validation.iloc[:,-1]    #target column \n",
        "    te_x = test.iloc[:,0:4]  #independent columns\n",
        "    te_y = test.iloc[:,-1]    #target column \n",
        "  oe = OneHotEncoder()\n",
        "  oe.fit(t_x.to_numpy()[:,:-1])\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  x_enc = oe.transform(t_x.to_numpy()[:,:-1]).toarray()\n",
        "  x_enc = np.append(x_enc, t_x.to_numpy()[:,-1].reshape(len(t_x),1) / 10, axis=1).astype(np.float32)\n",
        "\n",
        "  y_scaled = scaler.fit_transform(t_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  #VALID\n",
        "  vx_enc = oe.transform(v_x.to_numpy()[:,:-1]).toarray()\n",
        "  vx_enc = np.append(vx_enc, v_x.to_numpy()[:,-1].reshape(len(v_x),1) / 10, axis=1).astype(np.float32)\n",
        "  vy_scaled = scaler.fit_transform(v_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  #TEST\n",
        "  tex_enc = oe.transform(te_x.to_numpy()[:,:-1]).toarray()\n",
        "  tex_enc = np.append(tex_enc, te_x.to_numpy()[:,-1].reshape(len(te_x),1) / 10, axis=1).astype(np.float32)\n",
        "  tey_scaled = scaler.fit_transform(te_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  \n",
        "  model = create_linear_model(x_enc.shape[-1])\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n",
        "  rlr = ReduceLROnPlateau(patience=1,min_lr=0.00001)\n",
        "  history = model.fit(x_enc,y_scaled, validation_data=(vx_enc, vy_scaled) , batch_size=100,epochs=500,callbacks=[rlr,es],verbose=0)\n",
        "\n",
        "  #TEST RESULTS\n",
        "  prediction = model.predict(tex_enc)\n",
        "  prediction_after_scale = scaler.inverse_transform(prediction)\n",
        "  true_values = scaler.inverse_transform(tey_scaled)\n",
        "  test_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  test_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "\n",
        "  #TRAIN RESULTS\n",
        "  prediction = model.predict(x_enc)\n",
        "  prediction_after_scale = scaler.inverse_transform(prediction)\n",
        "  true_values = scaler.inverse_transform(y_scaled)\n",
        "  train_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  train_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "\n",
        "  #VALIDATION RESULTS\n",
        "  prediction = model.predict(vx_enc)\n",
        "  prediction_after_scale = scaler.inverse_transform(prediction)\n",
        "  true_values = scaler.inverse_transform(vy_scaled)\n",
        "  valid_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  valid_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "  return (train_r2,valid_r2,test_r2) , (train_mae,valid_mae,test_mae)\n",
        "\n",
        "def exp_poly(col_to_drop=None):\n",
        "  train , validation , test = prepare_data(col_to_drop)\n",
        "  if col_to_drop == None:\n",
        "    t_x = train.iloc[:,0:5]  #independent columns\n",
        "    t_y = train.iloc[:,-1]    #target column \n",
        "    v_x = validation.iloc[:,0:5]  #independent columns\n",
        "    v_y = validation.iloc[:,-1]    #target column \n",
        "    te_x = test.iloc[:,0:5]  #independent columns\n",
        "    te_y = test.iloc[:,-1]    #target column \n",
        "  else:\n",
        "    t_x = train.iloc[:,0:4]  #independent columns\n",
        "    t_y = train.iloc[:,-1]    #target column \n",
        "    v_x = validation.iloc[:,0:4]  #independent columns\n",
        "    v_y = validation.iloc[:,-1]    #target column \n",
        "    te_x = test.iloc[:,0:4]  #independent columns\n",
        "    te_y = test.iloc[:,-1]    #target column \n",
        "  oe = OneHotEncoder()\n",
        "  poly = PolynomialFeatures(2)\n",
        "  oe.fit(t_x.to_numpy()[:,:-1])\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  x_enc = oe.transform(t_x.to_numpy()[:,:-1]).toarray()\n",
        "  x_enc = np.append(x_enc, t_x.to_numpy()[:,-1].reshape(len(t_x),1) / 10, axis=1)\n",
        "  x_enc = poly.fit_transform(x_enc).astype(np.float32)\n",
        "  y_scaled = scaler.fit_transform(t_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  #VALID\n",
        "  vx_enc = oe.transform(v_x.to_numpy()[:,:-1]).toarray()\n",
        "  vx_enc = np.append(vx_enc, v_x.to_numpy()[:,-1].reshape(len(v_x),1) / 10, axis=1)\n",
        "  vx_enc = poly.fit_transform(vx_enc).astype(np.float32)\n",
        "  vy_scaled = scaler.fit_transform(v_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  #TEST\n",
        "  tex_enc = oe.transform(te_x.to_numpy()[:,:-1]).toarray()\n",
        "  tex_enc = np.append(tex_enc, te_x.to_numpy()[:,-1].reshape(len(te_x),1) / 10, axis=1)\n",
        "  tex_enc = poly.fit_transform(tex_enc).astype(np.float32)\n",
        "  tey_scaled = scaler.fit_transform(te_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  poly_input_size = vx_enc[0].shape[0]\n",
        "  model = create_linear_model(poly_input_size)\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n",
        "  rlr = ReduceLROnPlateau(patience=1,min_lr=0.00001)\n",
        "  history = model.fit(x_enc,y_scaled, validation_data=(vx_enc, vy_scaled) , batch_size=100,epochs=500,callbacks=[rlr,es],verbose=0)\n",
        "\n",
        "  #TEST RESULTS\n",
        "  prediction = model.predict(tex_enc)\n",
        "  prediction_after_scale = scaler.inverse_transform(prediction)\n",
        "  true_values = scaler.inverse_transform(tey_scaled)\n",
        "  test_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  test_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "\n",
        "  #TRAIN RESULTS\n",
        "  prediction = model.predict(x_enc)\n",
        "  prediction_after_scale = scaler.inverse_transform(prediction)\n",
        "  true_values = scaler.inverse_transform(y_scaled)\n",
        "  train_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  train_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "\n",
        "  #VALIDATION RESULTS\n",
        "  prediction = model.predict(vx_enc)\n",
        "  prediction_after_scale = scaler.inverse_transform(prediction)\n",
        "  true_values = scaler.inverse_transform(vy_scaled)\n",
        "  valid_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  valid_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "  return (train_r2,valid_r2,test_r2) , (train_mae,valid_mae,test_mae)\n",
        "\n",
        "def exp_sepEmbed(col_to_drop=None):\n",
        "  train , validation , test = prepare_data(col_to_drop)\n",
        "  if col_to_drop == None:\n",
        "    t_x = train.iloc[:,0:5]  #independent columns\n",
        "    t_y = train.iloc[:,-1]    #target column \n",
        "    v_x = validation.iloc[:,0:5]  #independent columns\n",
        "    v_y = validation.iloc[:,-1]    #target column \n",
        "    te_x = test.iloc[:,0:5]  #independent columns\n",
        "    te_y = test.iloc[:,-1]    #target column \n",
        "  else:\n",
        "    t_x = train.iloc[:,0:4]  #independent columns\n",
        "    t_y = train.iloc[:,-1]    #target column \n",
        "    v_x = validation.iloc[:,0:4]  #independent columns\n",
        "    v_y = validation.iloc[:,-1]    #target column \n",
        "    te_x = test.iloc[:,0:4]  #independent columns\n",
        "    te_y = test.iloc[:,-1]    #target column \n",
        "  oe = OrdinalEncoder()\n",
        "  oe.fit(t_x.to_numpy()[:,:-1])\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  x_enc = oe.transform(t_x.to_numpy()[:,:-1])#.toarray()\n",
        "  x_enc = np.append(x_enc, t_x.to_numpy()[:,-1].reshape(len(t_x),1) / 10, axis=1).astype(np.float32)\n",
        "\n",
        "  y_scaled = scaler.fit_transform(t_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  #VALID\n",
        "  vx_enc = oe.transform(v_x.to_numpy()[:,:-1])\n",
        "  vx_enc = np.append(vx_enc, v_x.to_numpy()[:,-1].reshape(len(v_x),1) / 10, axis=1).astype(np.float32)\n",
        "  vy_scaled = scaler.fit_transform(v_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  #TEST\n",
        "  tex_enc = oe.transform(te_x.to_numpy()[:,:-1])\n",
        "  tex_enc = np.append(tex_enc, te_x.to_numpy()[:,-1].reshape(len(te_x),1) / 10, axis=1).astype(np.float32)\n",
        "  tey_scaled = scaler.fit_transform(te_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n",
        "  rlr = ReduceLROnPlateau(patience=1,min_lr=0.00001)\n",
        "  if col_to_drop == None:\n",
        "    model = create_sep_embed_model()\n",
        "    history = model.fit(x=[x_enc[:,0],x_enc[:,1],x_enc[:,2],x_enc[:,3],x_enc[:,4]],y=y_scaled,\n",
        " validation_data=([vx_enc[:,0],vx_enc[:,1],vx_enc[:,2],vx_enc[:,3],vx_enc[:,4]], vy_scaled) ,\n",
        "  batch_size=100,epochs=500,callbacks=[rlr,es],verbose=0)\n",
        "    #TEST\n",
        "    test_prediction = model.predict(x=[tex_enc[:,0],tex_enc[:,1],tex_enc[:,2],tex_enc[:,3],tex_enc[:,4]])\n",
        "    train_prediction = model.predict(x=[x_enc[:,0],x_enc[:,1],x_enc[:,2],x_enc[:,3],x_enc[:,4]])\n",
        "    valid_prediction = model.predict(x=[vx_enc[:,0],vx_enc[:,1],vx_enc[:,2],vx_enc[:,3],vx_enc[:,4]])\n",
        "  else:\n",
        "    model = create_sep_embed_model_leave_one()\n",
        "    history = model.fit(x=[x_enc[:,0],x_enc[:,1],x_enc[:,2],x_enc[:,3]],y=y_scaled,\n",
        "      validation_data=([vx_enc[:,0],vx_enc[:,1],vx_enc[:,2],vx_enc[:,3]],vy_scaled) ,\n",
        "       batch_size=100,epochs=500,callbacks=[rlr,es],verbose=0)\n",
        "    #Test\n",
        "    test_prediction = model.predict(x=[tex_enc[:,0],tex_enc[:,1],tex_enc[:,2],tex_enc[:,3]])\n",
        "    train_prediction = model.predict(x=[x_enc[:,0],x_enc[:,1],x_enc[:,2],x_enc[:,3]])\n",
        "    valid_prediction = model.predict(x=[vx_enc[:,0],vx_enc[:,1],vx_enc[:,2],vx_enc[:,3]])\n",
        "\n",
        "  #TEST RESULTS\n",
        "  prediction_after_scale = scaler.inverse_transform(test_prediction)\n",
        "  true_values = scaler.inverse_transform(tey_scaled)\n",
        "  test_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  test_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "\n",
        "  #TRAIN RESULTS\n",
        "  prediction_after_scale = scaler.inverse_transform(train_prediction)\n",
        "  true_values = scaler.inverse_transform(y_scaled)\n",
        "  train_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  train_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "\n",
        "  #VALIDATION RESULTS\n",
        "  prediction_after_scale = scaler.inverse_transform(valid_prediction)\n",
        "  true_values = scaler.inverse_transform(vy_scaled)\n",
        "  valid_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  valid_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "  return (train_r2,valid_r2,test_r2) , (train_mae,valid_mae,test_mae)\n",
        "\n",
        "def exp_sharedEmbed(col_to_drop=None):\n",
        "  train , validation , test = prepare_data(col_to_drop)\n",
        "  if col_to_drop == None:\n",
        "    t_x = train.iloc[:,0:5]  #independent columns\n",
        "    t_y = train.iloc[:,-1]    #target column \n",
        "    v_x = validation.iloc[:,0:5]  #independent columns\n",
        "    v_y = validation.iloc[:,-1]    #target column \n",
        "    te_x = test.iloc[:,0:5]  #independent columns\n",
        "    te_y = test.iloc[:,-1]    #target column \n",
        "  else:\n",
        "    t_x = train.iloc[:,0:4]  #independent columns\n",
        "    t_y = train.iloc[:,-1]    #target column \n",
        "    v_x = validation.iloc[:,0:4]  #independent columns\n",
        "    v_y = validation.iloc[:,-1]    #target column \n",
        "    te_x = test.iloc[:,0:4]  #independent columns\n",
        "    te_y = test.iloc[:,-1]    #target column \n",
        "  x_before = t_x.astype('str').to_numpy()[:,:-1]\n",
        "  unique_values = pd.unique(t_x.astype('str').values.ravel()) \n",
        "  if col_to_drop is None:\n",
        "    oe = OrdinalEncoder(categories=[unique_values]*4)\n",
        "  else:\n",
        "    oe = OrdinalEncoder(categories=[unique_values]*3)\n",
        "  \n",
        "  x_enc = oe.fit_transform(x_before)\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  x_enc = np.append(x_enc, t_x.to_numpy()[:,-1].reshape(len(t_x),1) / 10, axis=1).astype(np.float32)\n",
        "  y_scaled = scaler.fit_transform(t_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "\n",
        "  #VALID\n",
        "  vx_before = v_x.astype('str').to_numpy()[:,:-1]\n",
        "  vx_enc = oe.transform(vx_before)\n",
        "  vx_enc = np.append(vx_enc, v_x.to_numpy()[:,-1].reshape(len(v_x),1) / 10, axis=1).astype(np.float32)\n",
        "  vy_scaled = scaler.fit_transform(v_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "  #TEST\n",
        "  tex_before = te_x.astype('str').to_numpy()[:,:-1]\n",
        "  tex_enc = oe.transform(tex_before)\n",
        "  tex_enc = np.append(tex_enc, te_x.to_numpy()[:,-1].reshape(len(te_x),1) / 10, axis=1).astype(np.float32)\n",
        "  tey_scaled = scaler.fit_transform(te_y.to_numpy().reshape(-1,1)).astype(np.float32)\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n",
        "  rlr = ReduceLROnPlateau(patience=1,min_lr=0.00001)\n",
        "  if col_to_drop == None:\n",
        "    model = create_embed_model(input_dim=4)\n",
        "    history = model.fit(x=[x_enc[:,:-1],x_enc[:,-1]],y=y_scaled,\n",
        " validation_data=([vx_enc[:,:-1],vx_enc[:,-1]], vy_scaled) , batch_size=100,epochs=500,callbacks=[rlr,es],verbose=0)\n",
        "    #TEST\n",
        "  else:\n",
        "    model = create_embed_model(input_dim=3)\n",
        "    history = model.fit(x=[x_enc[:,:-1],x_enc[:,-1]],y=y_scaled,\n",
        " validation_data=([vx_enc[:,:-1],vx_enc[:,-1]], vy_scaled) , batch_size=100,epochs=500,callbacks=[rlr,es],verbose=0)\n",
        "  test_prediction = model.predict(x=[tex_enc[:,:-1],tex_enc[:,-1]])\n",
        "  train_prediction = model.predict(x=[x_enc[:,:-1],x_enc[:,-1]])\n",
        "  valid_prediction = model.predict(x=[vx_enc[:,:-1],vx_enc[:,-1]])\n",
        "  #TEST RESULTS\n",
        "  prediction_after_scale = scaler.inverse_transform(test_prediction)\n",
        "  true_values = scaler.inverse_transform(tey_scaled)\n",
        "  test_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  test_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "\n",
        "  #TRAIN RESULTS\n",
        "  prediction_after_scale = scaler.inverse_transform(train_prediction)\n",
        "  true_values = scaler.inverse_transform(y_scaled)\n",
        "  train_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  train_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "\n",
        "  #VALIDATION RESULTS\n",
        "  prediction_after_scale = scaler.inverse_transform(valid_prediction)\n",
        "  true_values = scaler.inverse_transform(vy_scaled)\n",
        "  valid_r2 = r2_score(true_values,prediction_after_scale)\n",
        "  valid_mae = mean_absolute_error(true_values,prediction_after_scale)\n",
        "  return (train_r2,valid_r2,test_r2) , (train_mae,valid_mae,test_mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ecLlVi1aLQK",
        "outputId": "9ff83c1a-59b8-4a23-acba-b01c6dfdf75f"
      },
      "outputs": [],
      "source": [
        "results_without = {}\n",
        "for col in [None,'Sheeting_Type','Color','Orientation_(Degrees)','Observation_Angle_(Degrees)']:\n",
        "  for m in [('ohe' , exp_ohe), ('linear', exp_linear),('poly',exp_poly),('sharedEmbed',exp_sharedEmbed),('sepEmbed',exp_sepEmbed)]:\n",
        "    name , exp_mod = m\n",
        "    train_r2 = []\n",
        "    train_mae = []\n",
        "    valid_r2 = []\n",
        "    valid_mae = []\n",
        "    test_r2 = []\n",
        "    test_mae = []\n",
        "    for i in range(10):\n",
        "      r2, mae = exp_mod(col)\n",
        "      t , v, te = r2\n",
        "      train_r2.append(t)\n",
        "      valid_r2.append(v)\n",
        "      test_r2.append(te)\n",
        "      t , v , te = mae\n",
        "      train_mae.append(t)\n",
        "      valid_mae.append(v)\n",
        "      test_mae.append(te)\n",
        "    train_r2_results = str(np.mean(train_r2) ) + '±'+ str(np.std(train_r2))\n",
        "    train_mae_results = str(np.mean(train_mae) ) + '±'+ str(np.std(train_mae))\n",
        "    valid_r2_results = str(np.mean(valid_r2) ) + '±'+ str(np.std(valid_r2))\n",
        "    valid_mae_results = str(np.mean(valid_mae) ) + '±'+ str(np.std(valid_mae))\n",
        "    test_r2_results = str(np.mean(test_r2) ) + '±'+ str(np.std(test_r2))\n",
        "    test_mae_results = str(np.mean(test_mae) ) + '±'+ str(np.std(test_mae))\n",
        "    if col == None:\n",
        "      results_without['all'] = {'name' : name, 'train_r2' : train_r2_results,'train_mae' : train_mae_results,\n",
        "                                'valid_r2' : valid_r2_results, 'valid_mae' : valid_mae_results,\n",
        "                                'test_r2' : test_r2_results, 'test_mae' : test_mae_results}\n",
        "    else:\n",
        "      results_without[col] = {'name' : name, 'train_r2' : train_r2_results,'train_mae' : train_mae_results,\n",
        "                                'valid_r2' : valid_r2_results, 'valid_mae' : valid_mae_results,\n",
        "                                'test_r2' : test_r2_results, 'test_mae' : test_mae_results}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFPmNm5Aft6k"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"results.json\", \"w\") as outfile:\n",
        "    json.dump(results_without, outfile,indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOUGeZsWaayS",
        "outputId": "659a04f8-a513-4a53-c839-6ee7939dbf20"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"results.json\", \"r\") as infile:\n",
        "    x = json.load(infile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'all': {'name': 'sepEmbed',\n",
              "  'train_r2': '0.033146659319700164±0.4608042540338364',\n",
              "  'train_mae': '154.1158±57.803238',\n",
              "  'valid_r2': '0.013907833525415692±0.4104680848528115',\n",
              "  'valid_mae': '200.9431±47.181885',\n",
              "  'test_r2': '0.04395483680008565±0.45931889352632055',\n",
              "  'test_mae': '153.2675±57.534573'},\n",
              " 'Sheeting_Type': {'name': 'sepEmbed',\n",
              "  'train_r2': '0.5301517981127136±0.003886210519734137',\n",
              "  'train_mae': '129.28708±1.1719836',\n",
              "  'valid_r2': '0.7096173969307638±0.00883210334663149',\n",
              "  'valid_mae': '143.70761±5.1966963',\n",
              "  'test_r2': '0.7399618084730291±0.018810458338285676',\n",
              "  'test_mae': '85.939575±2.2473667'},\n",
              " 'Color': {'name': 'sepEmbed',\n",
              "  'train_r2': '0.3984964577185383±0.2165092123946608',\n",
              "  'train_mae': '143.05386±9.915717',\n",
              "  'valid_r2': '0.4716480053827798±0.24895640007902667',\n",
              "  'valid_mae': '175.72849±23.827394',\n",
              "  'test_r2': '0.09657925154477516±0.06826012735511487',\n",
              "  'test_mae': '198.68674±18.49206'},\n",
              " 'Orientation_(Degrees)': {'name': 'sepEmbed',\n",
              "  'train_r2': '0.49652694040188783±0.5250987334451671',\n",
              "  'train_mae': '98.442184±58.973743',\n",
              "  'valid_r2': '0.4214885313295224±0.45882064166706965',\n",
              "  'valid_mae': '157.2404±52.4103',\n",
              "  'test_r2': '0.5072457589659163±0.5231531889251455',\n",
              "  'test_mae': '96.7342±59.744987'},\n",
              " 'Observation_Angle_(Degrees)': {'name': 'sepEmbed',\n",
              "  'train_r2': '0.2940596326551245±0.25535578938980497',\n",
              "  'train_mae': '145.87416±20.492052',\n",
              "  'valid_r2': '0.19298657521126947±0.17407966939802175',\n",
              "  'valid_mae': '217.96829±12.740967',\n",
              "  'test_r2': '0.3107205841014488±0.26042133893466163',\n",
              "  'test_mae': '149.56046±16.967806'}}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "retroreflection.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "4183f179fa4004da4aa44c36ef410e8cbd0aa3feb647821d85acfad69fafc31e"
    },
    "kernelspec": {
      "display_name": "Python 3.6.10 64-bit ('ics535Research': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
